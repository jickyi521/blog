---
title: FIRE-183期-20250216
date: 2025-02-16 19:42:13
updated: 2025-02-16 19:42:13
tags:
- FIRE
categories:
- 投资
---

![b25a483a008d499793472e5fa7899c8e.jpg](https://s2.loli.net/2025/02/16/Aov4JgYbeOM2LFm.jpg) 

## 本周数据

### 概要

| 投入   |-20170                                                  |
| ------ | ------------------------------------------------------------ |
| 收益率 | 本周：<font color="red" size=4>  3.61% </font> / `沪深300：1.19%`    <br />本年：<font color="red" size=4> 3.92% </font>/ `沪深300:0.33%` |
| 账本   | <img src="https://s2.loli.net/2025/02/16/f6OgBkwq24SZPc1.jpg" style="zoom:33%;" /> |
| 消费   | 979                                           |

### 交易

* 支付宝基金账户转出 20170，减些仓位到余额宝 

## 感想

 DeepSeek 降本的关键点

 1. MoE （mixture of experts）混合专家模型，显著降低训练和推理的计算成本
```
 模型预设一组专家，在训练或推理时，只需要激活一部分参数，会自动学习针对不同任务路由到哪个专家.

 DeepSeek 的模型拥有超过 6000 亿个参数，相⽐之下，Llama 405B 有 4050 亿参数，
Llama 70B 有 700 亿参数。从参数规模上看，DeepSeek 模型拥有更⼤的信息压缩空间，可以
容纳更多来⾃互联⽹的世界知识。但与此同时，模型每次只激活约 370 亿个参数。也就是说，在
训练或推理过程中，只需要计算 370 亿个参数。相⽐之下，Llama 模型每次推理都需要激活
700 亿或 4050 亿个参数。因此，采⽤混合专家架构可以显著降低训练和推理的计算成本。

```

 2. MLA（Multi-Modal Learning Architecture）多模态学习架构，大幅度降低内存压力

```
⽤于减少推理过程中的内存占⽤，在 CUDA 层甚至更底层做编码。

通过 CUDA 内核级优化实现动态路由加速，相较于常规框架级实现获得 23% 的吞吐量提升，
整体占用比传统架构降低 67%-90%

```


## 好内容

1. [DeepSeek 小传](https://www.xiaoyuzhoufm.com/episode/67ac24df5e6f58876dd4362f)
[DeepSeek 小传：制造了 AI 拐点的科技苦旅](https://mp.weixin.qq.com/s/prmwOtDmhHZrsC0xxE_vRQ)

2. [再议县城：什么是「县感」、城迁群像与转移支付体系](https://www.xiaoyuzhoufm.com/episode/67ac07ed2faa72d7a8fc2bb2)

3. [从现在起，努力为自己打造一条永续现金流](https://www.xiaoyuzhoufm.com/episode/67adba76606e5c59403c7a53)